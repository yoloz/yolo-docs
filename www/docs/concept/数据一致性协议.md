## 两阶段提交协议（2PC）

:::info
二阶段提交( Two-phaseCommit )是指，在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法( Algorithm )。通常，二阶段提交也被称为是一种协议( Protocol )。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的 ACID 特性，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，二阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。
:::

### 两种角色

- 协调者
- 参与者

### 处理的二个阶段

- 询问投票阶段：事务协调者给每个参与者发送 Prepare 消息，参与者受到消息后，要么在本地写入 redo 和 undo 日志成功后，返回同意的消息，否者一个终止事务的消息。
- 执行初始化（执行提交）：协调者在收到所有参与者的消息后，如果有一个返回终止事务，那么协调者给每个参与者发送回滚的指令。否者发送 commit 消息

### 异常情况处理

- 协调者故障：备用协调者接管，并查询参与者执行到什么地址
- 参与者故障：协调者会等待他重启然后执行
- 协调者和参与者同时故障：协调者故障，然后参与者也故障。例如：有机器 1，2，3，4。其中 4 是协调者，1，2，3 是参与者 4 给 1，2 发完提交事务后故障了，正好 3 这个时候也故障了，注意这时 3 是没有提交事务数据的。在备用协调者启动了，去询问参与者，由于 3 死掉了，一直不知道它处于什么状态（接受了提交事务，还是反馈了能执行还是不能执行 3 个状态）。面对这种情况，2PC，是不能解决的，要解决需要下文介绍的 3PC。

### 缺点

- **同步阻塞问题**：由于所有参与的节点都是事务阻塞型的，例如 `update table set status=1 where current_day=20181103`，那么参与者 table 表的 current_day=20181103 的记录都会被锁住，其他的要修改 current_day=20181103 行的事务，都会被阻塞
- **单点故障阻塞其他事务**：协调者在执行提交的阶段 down 掉，所有的参与者出于锁定事务资源的状态中。无法完成相关的事务操作。
- **参与者和协调者同时 down 掉**：协调者在发送完 commit 消息后 down 掉，而唯一接受到此消息的参与者也 down 掉了。新协调者接管，也是一个懵逼的状态，不知道此条事务的状态。无论提交或者回滚都是不合适的。这个是两阶段提交无法改变的

## 三阶段提交协议（3PC）

2PC 当时只考虑如果单机故障的情况，是可以勉强应付的。当遇到协调者和参与者同时故障的话，2PC 的理论是不完善的。此时 3PC 登场。

3PC 就是对 2PC 漏洞的补充协议。主要改动两点：

:::tip

- 在 2PC 的第一阶段和第二阶段插入一个准备阶段，做到就算参与者和协调者同时故障也不阻塞，并且保证一致性;
- 在协调者和参与者之间引入超时机制;

:::

### 处理的三个阶段

- 事务询问阶段（ can commit 阶段)：协调者向参与者发送 commit 请求，然后等待参与者反应。这个和 2PC 阶段不同的是，此时参与者没有锁定资源，没有写 redo，undo，执行回滚日志。**回滚代价低**
- 事务准备阶段 （pre commit）：如果参与者都返回 ok，那么就发送 Prepare 消息，参与者本地执行 redo 和 undo 日志。否者就向参与者提交终止（abort）事务的请求。如果再发送 Prepare 消息的时候，等待超时，也会向参与者提交终止事务的请求。
- 执行事务阶段（do commit）：如果所有发送 Prepare 都返回成功，那么此时变为执行事务阶段，向参与者发送 commit 事务的消息。否者回滚事务。在此阶段参与者如果在一定时间内没有收到 docommit 消息，触发超时机制，会自己提交事务。此番处理的逻辑是，能够进入此阶段，说明在事务询问阶段所有节点都是好的。即使在提交的时候部分失败，有理由相信，此时大部分节点都是好的。是可以提交的

### 缺点

- 不能解决网络分区的导致的数据不一致的问题：例如 1~5 五个参与者节点，1，2，3 个节点在 A 机房，4，5 节点在 B 机房。在 pre commit 阶段，1~5 个节点都收到 Prepare 消息，但是节点 1 执行失败。协调者向 1~5 个节点发送回滚事务的消息。但是此时 A，B 机房的网络分区。1~3 号节点会回滚。但是 4~5 节点由于没收到回滚事务的消息，而提交了事务。待网络分区恢复后，会出现数据不一致的情况。
- 不能解决 fail-recover 的问题：由于 3PC 有超时机制的存在，2PC 中未解决的问题，参与者和协调者同时 down 掉，也就解决了。一旦参与者在超时时间内没有收到协调者的消息，就会自己提交。这样也能避免参与者一直占用共享资源。但是其在网络分区的情况下，不能保证数据的一致性。

## Paxos 协议

像 2PC 和 3PC 都需要引入一个协调者的角色，当协调者 down 掉之后，整个事务都无法提交，参与者的资源都出于锁定的状态，对于系统的影响是灾难性的，而且出现网络分区的情况，很有可能会出现数据不一致的情况。有没有不需要协调者角色，每个参与者来协调事务呢，在网络分区的情况下，又能最大程度保证一致性的解决方案呢。此时 Paxos 出现了。

Paxos 算法是 Lamport 于 1990 年提出的一种基于消息传递的一致性算法。由于算法难以理解起初并没有引起人们的重视，Lamport 在八年后重新发表，即便如此 Paxos 算法还是没有得到重视。2006 年 Google 的三篇论文石破天惊，其中的 chubby 锁服务使用 Paxos 作为 chubbycell 中的一致性，后来才得到关注。

### 解决了什么问题

Paxos 协议是一个解决分布式系统中，多个节点之间就某个值（提案）达成一致（决议）的通信协议。它能够处理在少数节点离线的情况下，剩余的多数节点仍然能够达成一致。即每个节点，既是参与者，也是决策者

### 两种角色（两者可以是同一台机器）

- Proposer：提议提案的服务器
- Acceptor：批准提案的服务器

## Raft 协议

Paxos 是论证了一致性协议的可行性，但是论证的过程据说晦涩难懂，缺少必要的实现细节，而且工程实现难度比较高广为人知实现只有 [zookeeper 的实现 zab 协议](../bigdata/zookeeper学习.md)。然后斯坦福大学 RamCloud 项目中提出了易实现，易理解的分布式一致性复制协议 Raft。Java，C++，Go 等都有其对应的实现。

### 基本名词

- 节点状态
- Leader（主节点）：接受 client 更新请求，写入本地后，然后同步到其他副本中
- Follower（从节点）：从 Leader 中接受更新请求，然后写入本地日志文件。对客户端提供读请求
- Candidate（候选节点）：如果 follower 在一段时间内未收到 leader 心跳。则判断 leader 可能故障，发起选主提议。节点状态从 Follower 变为 Candidate 状态，直到选主结束
- termId：任期号，时间被划分成一个个任期，每次选举后都会产生一个新的 termId，一个任期内只有一个 leader。termId 相当于 paxos 的 proposalId。
- RequestVote：请求投票，candidate 在选举过程中发起，收到 quorum (多数派）响应后，成为 leader。
- AppendEntries：附加日志，leader 发送日志和心跳的机制
- election timeout：选举超时，如果 follower 在一段时间内没有收到任何消息(追加日志或者心跳)，就是选举超时。

### 特性

- Leader 不会修改自身日志，只会做追加操作，日志只能由 Leader 转向 Follower。例如即将要 down 掉的 Leader 节点已经提交日志 1，未提交日志 2，3。down 掉之后，节点 2 启动最新日志只有 1，然后提交了日志 4。好巧不巧节点 1 又启动了。此时节点 2 的编号 4 日志会追加到节点 1 的编号 1 日志的后面。节点 1 编号 2，3 的日志会丢掉。
- 不依赖各个节点物理时序保证一致性，通过逻辑递增的 term-id 和 log-id 保证。

### 选主契机

- 在超时时间内没有收到 Leader 的心跳
- 启动时

### 选主过程

![raft1.jpeg](/docs/concept/raft1.jpeg)

如图所示，Raft 将时间分为多个 term（任期），term 以连续的整数来标识，每个 term 表示一个选举的开始。例如 Follower 节点 1，在 term1 和 term2 连接处的时间，联系不到 Leader，将 currentTerm 编号加 1，变成 2，进入了到 term2 任期，在 term2 的蓝色部分选举完成，绿色部分正常工作。

当然一个任期不一定能选出 Leader，那么会将 currentTerm 继续加 1，然后继续进行选举，例如图中的 t3。选举的原则是，每一轮选举每个选民一张选票，投票的请求先到且选民发现候选人节点的日志 id 大于等于自己的，就会投票，否者不会投票。获得半数以上的票的节点成为主节点，**注意**这并不是说选出来的事务 id 一定是最大的。例如 a~f 六个节点（正方形框里面的数字是选举的轮数 term）,在第四轮选举中，a 先发出投票，六台机器中，a~e 都会投 a，即使 f 不投 a，a 也会赢得选举。如果没有事务 id（如刚启动时），就遵循投票请求先来先头。然后 Leader 将最新的日志复制到各个节点，再对外提供服务。

当然除了这些选举限制，还会有其他的情况。如 commit 限制等保证，Leader 选举成功一定包含所有的 commit 和 log。

### 日志复制过程

![raft2.png](/docs/concept/raft2.png)

raft 日志写入过程，主节点收到一个 x=1 的请求后，会写入本地日志，然后将 x=1 的日志广播出去，follower 如果收到请求，会将日志写入本地 log ，然后返回成功。当 leader 收到半数以上的节点回应时，会将此日志的状态变为 commit，然后广播消息让 follwer 提交日志。节点在 commit 日志后，会更新状态机中的 logindex 。

firstLogIndex/lastLogIndex 为节点中开始和结束的索引位置（包含提交，未提交，写入状态机）commitIndex：已提交的索引。applyIndex：已写入状态机中的索引。

:::info
日志复制的本质是让 follwer 和 Leader 的已提交的日志顺序和内容都完全一样，用于保证一致性
:::

具体的原则就是：

原则 1：两个日志在不同的 raft 节点中，如果有两个相同的 term 和 logIndex ，则保证两个日志的内容完全一样。

原则 2：两段日志在不同的 raft 节点中，如果起始和终止的的 term，logIndex 都相同，那么两段日志中日志内容完全一样。

如何保证

第一个原则只需要在创建 logIndex 的时候使用新的 logIndex，保证 logIndex 的唯一性。而且创建之后不去更改。那么在 leader 复制到 follwer 之后，logIndex，term 和日志内容都没变。

第二个原则，在 Leader 复制给 Follower 时，要传递当前最新日志 currenTermId 和 currentLogIndex，以及上一条日志 preCurrentTermId 和 preCurrentLogIndex。如上图，在 d 节点，term7，logIndex12。在给节点节点 a 同步时，发送(term7，logIndex11)，(term7，logIndex12),a 节点没有找到(term7，logIndex11)的日志，会让 Leader，d 节点重新发送。d 节点会重新发（term6，logIndex10）(term7，logIndex11)，还是没有(term6，logIndex10)的日志,依然会拒绝同步。接着发(term6，logIndex9)(term6，logIndex10)。现在 a 节点有了(term6，logIndex9)。那么 leader 节点就会将(term6，logIndex9) ~ (term7，logIndex11)日志内容给节点 a，节点 a 将会和节点 d 有一样的日志。

## 使用 Raft + RocksDB 有赞分布式 KV 存储服务

当前开源的缓存 kv 系统，大都是 AP 系统，例如设置主从同步集群 redis，master 异步同步到 slave。虽然在 master 停止服务后，slave 会顶上来。但是在 master 写入了数据，但是还没来得及同步到 slave 就 down 了，然后 slave 被选为主节点继续对外提供服务的情况下，会丢失部分数据。这对于要求强一致性的系统来说是不可接受的。例如很多场景下 redis 做分布式锁，有天然的缺陷在里面，如果 master 停止服务，这个锁不很不可靠的，虽然出现的几率很小，但一旦出现，将是致命的错误。

为了实现 CP 的 KV 存储系统，且要兼容现有的 redis 业务。有赞开发了 ZanKV（现已开源 ZanRedisDB）。

![zankv1.jpeg](/docs/concept/zankv1.jpeg)

底层的存储结构是 RocksDB（底层采用 LSM 数据结构）。一个 setx=1 的会通过 redis protocol 协议传输，内容会通过 Raft 协议，同步写入到其他的节点的 RocksDB。有了 Raft 理论的加持，RocksDB 优秀的存储性能，即使遇到网络分区，master 节点 down 掉， slave 节点 down 掉，等一系列异常情况，其都能轻松应对。在扩容方面，系统用选择维护映射表的方式来建立分区和节点的关系，映射表会根据一定的算法并配合灵活的策略生成，来达到方便扩容。具体原理可参见[《使用开源技术构建有赞分布式 KV 存储服务》](http://mp.weixin.qq.com/s?__biz=MzAxOTY5MDMxNA==&mid=2455758863&idx=1&sn=f36780332396801b4acc975c4b8e1359&chksm=8c686e2abb1fe73c156d0a6b7efff4042b4d723bc14a153f45a5bf983e5fae0240c03f2336de&scene=21#wechat_redirect)。

## 总结

为了保证数据 commit 之后不可丢，系统都会采用（WAL write ahead log）（在每次修改数据之前先写操作内容日志，然后再去修改数据。即使修改数据时异常，也可以通过操作内容日志恢复数据）

分布式存储系统中，是假设机器是不稳定，随时都有可能 down 掉的情况下来设计的。也就是说就算机器 down 掉了，用户写入的数据也不能丢，避免单点故障。为此每一份写入的数据，需要在多个副本中同时存放。例如 zk 节点数据复制，etcd 的数据复制。而复制数据给节点又会带来一致性的问题，例如主节点和从节点数据不一致改如何去同步数据。也会带来可用性的问题，如 leader 节点 down 掉，如何快速选主，恢复数据等。好在已有成熟的理论如 Paxos 协议，ZAB 协议 Raft 协议等做为支撑。
